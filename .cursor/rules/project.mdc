---
alwaysApply: true
---

# Cursor Agent Rules

# Do's and Don'ts
- OpenAPI client auto-regenerates on code changes when dev servers are running - don't manually regenerate.
- Prefer running apx related commands via MCP server if it's available.
- Use the apx MCP `search_registry_components` and `add_component` tools to find and add shadcn/ui components.
- When using the API calls on the frontend, use error boundaries to handle errors.
- Run `apx dev check` command (via CLI or MCP) to check for errors in the project code after making changes.
- If agent has access to native browser tool, use it to verify changes on the frontend. If such tool is not present or is not working, use playwright MCP to automate browser actions (e.g. screenshots, clicks, etc.).
- Avoid unnecessary restarts of the development servers
- **Databricks SDK:** Use the apx MCP `docs` tool to search Databricks SDK documentation instead of guessing or hallucinating API signatures.
- Always use Databricks latest features, releases and naming
- When mentioning an APP in a prompt, always confirm if it is referring ot a Databricks APP

## Package Management
- **Frontend:** Bun might not be present on user's $PATH. It's recommended to use prebundled bun (e.g., `uv run apx bun install` or `uv run apx bun add <dependency>`), unless user explicitly stated otherwise.
- **Python:** Always use `uv` (never `pip`)

## Component Management
- **Finding components:** Use MCP `search_registry_components` to search for available shadcn/ui components
- **Adding components:** Use MCP `add_component` or CLI `uv run apx components add <component> --yes` to add components
- **Component location:** If component was added to a wrong location (e.g. stored into `src/components` instead of `src/payment-analysis/ui/components`), move it to the proper folder
- **Component organization:** Prefer grouping components by functionality rather than by file type (e.g. `src/payment-analysis/ui/components/chat/`)

## Project Structure
Full-stack app: `src/payment-analysis/ui/` (React + Vite) and `src/payment-analysis/backend/` (FastAPI). Backend serves frontend at `/` and API at `/api`. API client auto-generated from OpenAPI schema.


## Models & API
- **3-model pattern:** `Entity` (DB), `EntityIn` (input), `EntityOut` (output)
- **API routes must have:** `response_model` and `operation_id` for client generation

## Frontend Rules
- **Routing:** `@tanstack/react-router` (routes in `src/payment-analysis/ui/routes/`)
- **Data fetching:** Always use `useXSuspense` hooks with `Suspense` and `Skeleton` components
- **Pattern:** Render static elements immediately, fetch API data with suspense
- **Components:** Use shadcn/ui, add to `src/payment-analysis/ui/components/`
- **Data access:** Use `selector()` function for clean destructuring (e.g., `const {data: profile} = useProfileSuspense(selector())`)

## Development Commands

**Start dev servers** (backend, frontend, OpenAPI watcher):
```bash
uv run apx dev start
```

**Check status** (shows running servers and ports):
```bash
uv run apx dev status
```

**Check for errors** (TypeScript, Python linting):
```bash
uv run apx dev check
```

**Verify all** (lint, build, backend smoke test, dashboard assets, bundle validate):
```bash
./scripts/verify_all.sh [dev|prod]
```

**View logs:**
```bash
uv run apx dev logs              # Recent logs (default: last 10m)
uv run apx dev logs -d 1h        # Logs from last hour
uv run apx dev logs -f           # Follow/stream logs live
```

**Stop servers:**
```bash
uv run apx dev stop
```

**Build for production:**
```bash
uv run apx build
```

**Deploy to Databricks** (run from repo root; prepares dashboards then deploys):
```bash
./scripts/deploy.sh [dev|prod]
# Or: ./scripts/validate_bundle.sh dev && databricks bundle deploy -t dev
```
Before any `databricks bundle` command (validate, deploy, run), ensure dashboard files exist: run `./scripts/ensure_dashboards.sh [dev|prod]` or `uv run python scripts/prepare_dashboards.py`â€”otherwise you get "failed to read serialized dashboard from file_path .build/dashboards/...."

**Note:** OpenAPI client is automatically regenerated on every code change when dev servers are running. No manual regeneration needed.

## MCP Tools Reference

When the apx MCP server is available, use these tools:

| Tool | Description |
|------|-------------|
| `start` | Start development server and return the URL |
| `stop` | Stop the development server |
| `restart` | Restart the development server (preserves port if possible) |
| `logs` | Fetch recent dev server logs |
| `check` | Check project code for errors (runs tsc and ty checks in parallel) |
| `refresh_openapi` | Regenerate OpenAPI schema and API client |
| `search_registry_components` | Search shadcn registry components using semantic search |
| `add_component` | Add a component to the project |
| `docs` | Search Databricks SDK documentation for code examples and API references |
| `databricks_apps_logs` | Fetch logs from deployed Databricks app using Databricks CLI |
| `get_route_info` | Get code example for using a specific API route |
