# 4. Technical

**Payment Analysis** — architecture, bundle resources, and deployment reference.

## Architecture

- **Databricks:** Simulator → **Lakeflow** (Bronze → Silver → Gold) → Unity Catalog (12+ views, 4 models). MLflow, Model Serving, Mosaic AI Gateway, Genie. 12 AI/BI dashboards, SQL Warehouse.
- **App:** FastAPI (analytics, decisioning, notebooks, dashboards, agents) ↔ React (dashboard, dashboards, notebooks, models, ai-agents, decisioning, experiments, declines).

## Data Layer

Bronze: raw + `_ingested_at`. Silver: quality, `risk_tier`, `amount_bucket`, `composite_risk_score`. Gold: 12+ views. UC: `ahs_demos_catalog.ahs_demo_payment_analysis_dev` — governance, lineage, audit.

**Lakehouse recommendations & Vector Search:** `vector_search_and_recommendations.sql` defines `transaction_summaries_for_search` (Delta, source for Vector Search index), `approval_recommendations`, and view `v_recommendations_from_lakehouse`. The app’s **Decisioning** page shows a “Similar cases & recommendations (Lakehouse)” card fed by `GET /api/analytics/recommendations`, which reads from this view to accelerate approval decisions (e.g. similar transactions, retry suggestions). Vector Search index (`resources/vector_search.yml`) syncs from the search table using `databricks-bge-large-en`; when populated, it can power similar-transaction lookups and RAG for agents.

**Approval rules (Lakehouse):** `approval_rules.sql` defines `approval_rules` and view `v_approval_rules_active`. The app **Rules** page (sidebar) lets users create, edit, and delete rules stored in this table. ML and AI agents (e.g. Smart Routing, Smart Retry, Orchestrator) can read from `catalog.schema.approval_rules` or `v_approval_rules_active` to apply business rules and accelerate approval rates alongside model outputs. API: `GET/POST /api/rules`, `PATCH/DELETE /api/rules/{id}`.

**Online features (Lakehouse):** `online_features.sql` defines `online_features` and view `v_online_features_latest`. Features from ML and AI processes are stored here; the app **Dashboard** shows them in the "Online features (Lakehouse)" card. API: `GET /api/analytics/online-features` (optional `source=ml|agent`, `limit`). Populate from decisioning, model serving, or agent jobs.

**Catalog and schema (app_config):** The effective Unity Catalog and schema used for all Lakehouse operations (analytics, rules, ML, views) can be set via the **Setup & Run** page. Values are stored in a single-row table `app_config` (see `src/payment_analysis/transform/app_config.sql`) in the bootstrap catalog/schema (from `DATABRICKS_CATALOG` / `DATABRICKS_SCHEMA`). At startup the app reads this table; if missing or empty it falls back to env. The UI can update catalog/schema with **Save catalog & schema** (`PATCH /api/setup/config`), which writes to the table and refreshes app-wide config so all subsequent requests use the new values.

## ML Layer

Models: approval propensity (RF ~92%), risk (~88%), routing (RF ~75%), retry (~81%). MLflow → UC Registry → Serving (<50ms p95, scale-to-zero). Lifecycle: experiment → register → serve → monitor → retrain.

## AI Agents (Summary)

Genie 2, Model serving 3, Mosaic AI Gateway (LLM) 2. Agent jobs: [agents.yml](../resources/agents.yml). Details and **using Agent Bricks / Mosaic AI to accelerate approval rates:** [3_AGENTS_VALUE](3_AGENTS_VALUE.md#accelerating-approval-rates-with-agent-bricks-and-mosaic-ai).

## Analytics

12 dashboards are defined in `resources/dashboards.yml` and deployed from `.build/dashboards/*.lvdash.json` (generated by `scripts/prepare_dashboards.py` with `var.catalog`/`var.schema`). The **Create Gold Views** job runs `.build/transform/gold_views.sql` (with `USE CATALOG`/`USE SCHEMA`) so views exist in the same catalog.schema. Required assets: `uv run python scripts/validate_dashboard_assets.py`. Warehouse from bundle `var.warehouse_id`. Genie: spaces use catalog/schema; see `genie_spaces.yml`.

## Application Layer

**Backend:** `/api/analytics`, `/api/decisioning`, `/api/notebooks`, `/api/dashboards`, `/api/agents`, `/api/rules`, `/api/setup`. UC via Databricks SQL. **Frontend:** React + TanStack Router; `lib/api.ts`. **Stack:** Delta, UC, Lakeflow, SQL Warehouse, MLflow, Serving, Dashboards, Genie, Mosaic AI Gateway, FastAPI, React, TypeScript, Bun, TailwindCSS, Databricks Asset Bundles.

## Verification (test, validate, verify)

Run all local checks (TypeScript + Python, build, backend smoke test, dashboard assets, bundle validate):

```bash
./scripts/verify_all.sh [dev|prod]
```

Individual steps: `uv run apx dev check` (lint/types), `uv run apx build` (production build), `uv run python -c "from payment_analysis.backend.app import app; from payment_analysis.backend.router import api"` (backend import), `uv run python scripts/validate_dashboard_assets.py` (dashboard dependencies), `./scripts/validate_bundle.sh dev` (bundle validate). Best practice: run `./scripts/verify_all.sh dev` before committing or deploying.

## Lakebase (managed Postgres) deployment

[Lakebase](https://www.databricks.com/product/lakebase) is Databricks’ managed Postgres for operational data (rules, experiments, incidents). It is separate from the Lakehouse (Unity Catalog / Delta).

The bundle **deploys a Lakebase instance** via **`resources/lakebase.yml`**:

- **`database_instances.payment_analysis_db`** — one Lakebase Provisioned instance (name from `var.lakebase_instance_name`, default `payment-analysis-db`; capacity `var.lakebase_capacity`, default `CU_1`).
- **`database_catalogs.payment_analysis_lakebase_catalog`** — registers that instance as a Unity Catalog catalog (`var.lakebase_uc_catalog_name`) so you can query it from the lakehouse.

Variables (in `databricks.yml`): `lakebase_instance_name`, `lakebase_capacity` (e.g. `CU_1`), `lakebase_uc_catalog_name`, `lakebase_database_name`. After deploy, set the app env **`PGAPPNAME`** to the instance name (e.g. `payment-analysis-db`) so the app uses this DB for rules, experiments, and incidents.

## Databricks App (deploy)

The app is deployed as a **Databricks App** (FastAPI + React). **Pre-installed Python libraries** (do not add to requirements.txt):

| Library | Version |
|---------|---------|
| databricks-sql-connector | 3.4.0 |
| databricks-sdk | 0.33.0 |
| mlflow-skinny | 2.16.2 |
| gradio | 4.44.0 |
| streamlit | 1.38.0 |
| shiny | 1.1.0 |
| dash | 2.18.1 |
| flask | 3.0.3 |
| fastapi | 0.115.0 |
| uvicorn[standard] | 0.30.6 |
| gunicorn | 23.0.0 |
| huggingface-hub | 0.35.3 |
| dash-ag-grid | 31.2.0 |
| dash-mantine-components | 0.14.4 |
| dash-bootstrap-components | 1.6.0 |
| plotly | 5.24.1 |
| plotly-resampler | 0.10.0 |

**requirements.txt** adds only packages *not* in the table above: `pydantic-settings==2.6.1`, `sqlmodel==0.0.27`, `psycopg==3.2.3` (pure Python; no `[binary]` to avoid build in container). See **Databricks Apps package compatibility** below.

**Lakebase database:** Optional. Uncomment resources/lakebase.yml and use a unique lakebase_instance_name; then set **`PGAPPNAME`** in the app environment to that name. If unset, the app starts without DB and rules/experiments/incidents endpoints return 503. Local dev uses `APX_DEV_DB_PORT` instead.

**Runtime:** `app.yaml` sets `command` (uvicorn), `PYTHONPATH=src`, and one worker. After deploy, open the app from Workspace → Apps.

**Version strategy (Databricks App deployment):** Pin versions for reproducibility and to avoid drift. **Python:** Overriding a pre-installed Python package with a specific version is supported and recommended when stability matters. `requirements.txt` (App runtime) and `pyproject.toml` (local dev) share the same pins for overlapping packages (`pydantic-settings`, `sqlmodel`, `psycopg`). **Node:** `package.json` uses exact pinned versions (no `^`); lockfile is the single source of resolved versions. Always test after version changes (e.g. `uv run apx dev check`, `uv run apx build`, then deploy).

**Databricks Apps package compatibility** — App runtime: **Python 3.11**, **Node.js 22.16**, Ubuntu 22.04 ([system env](https://docs.databricks.com/en/dev-tools/databricks-apps/system-env)). **You cannot install system-level packages** (e.g. via `apt-get` or Conda)—app dependencies are restricted to **requirements.txt** (Python) and **package.json** (Node). **Python:** Only add packages not pre-installed; use pinned versions and pure-Python drivers (e.g. `psycopg` not `psycopg[binary]`) so `pip install` succeeds without a C compiler or system libs. **Node:** Anything required for `npm run build` must be under **dependencies** (not devDependencies), because with `NODE_ENV=production` the platform may skip installing devDependencies ([Manage dependencies](https://docs.databricks.com/en/dev-tools/databricks-apps/dependencies)). No Node libraries are pre-installed. Resolve TanStack peer conflicts by aligning versions: keep `@tanstack/react-router`, `@tanstack/router-plugin`, and `@tanstack/react-router-devtools` on the same major.minor (e.g. all **1.158.1**) and add **overrides** in `package.json` (e.g. `"overrides": { "@tanstack/react-router": "1.158.1", "@tanstack/router-plugin": "1.158.1" }`) so Databricks App's `npm install` resolves a single version and avoids ERESOLVE; do not use `--force` or `--legacy-peer-deps`. After version changes, clear and reinstall to ensure a clean graph (e.g. delete `node_modules` and lockfile, then `npm install` or `bun install`). Verify with `npm ls @tanstack/react-router` that a single compatible version is installed.

## Bundle & Deploy

**Included by default (deploy succeeds without optional resources):**

| Resource file | Contents |
|---------------|----------|
| `unity_catalog.yml` | UC schema + 4 volumes (raw_data, checkpoints, ml_artifacts, reports) |
| `pipelines.yml` | Payment Analysis ETL, Real-Time Stream (Lakeflow) |
| `sql_warehouse.yml` | Payment Analysis Warehouse (PRO, serverless) |
| `ml_jobs.yml` | Train ML Models, Create Gold Views, Test Agent Framework |
| `agents.yml` | 6 agent jobs (Smart Routing, Smart Retry, Decline Analyst, Risk Assessor, Performance Recommender, Orchestrator) |
| `ai_gateway.yml` | Mosaic AI Gateway (governed LLM) |
| `streaming_simulator.yml` | Transaction Stream Simulator job, Continuous Stream Processor job |
| `genie_spaces.yml` | Genie Space Sync job |
| `dashboards.yml` | 12 AI/BI dashboards |
| `app.yml` | Databricks App (payment-analysis) |

**Optional (commented out in databricks.yml):** `lakebase.yml` — use a unique `lakebase_instance_name` (e.g. `--var lakebase_instance_name=payment-analysis-db-YOURNAME`) to avoid "Instance name is not unique"; then set app **PGAPPNAME**. `model_serving.yml` — run Step 6 (Train ML Models) first so 4 models exist in UC, then uncomment and redeploy.

**Not in bundle (create manually if needed):** Vector Search endpoint and index are not in the Asset Bundle schema yet. See `resources/vector_search.yml` for the spec and create the endpoint/index in the Vector Search UI, or via API, after running `vector_search_and_recommendations.sql`.

**Variables:** `catalog`, `schema`, `environment`, `warehouse_id`, `lakebase_*`, `workspace_folder` (default `payment-analysis`; workspace root path). **Commands:** `./scripts/deploy.sh dev` (or `validate_bundle.sh dev` then `databricks bundle deploy -t dev`); prod: `./scripts/deploy.sh prod`. **App:** `.env` (DATABRICKS_HOST, TOKEN, WAREHOUSE_ID, optional DATABRICKS_CATALOG/SCHEMA); set **PGAPPNAME** to Lakebase instance name for rules/experiments/incidents. Set **BUNDLE_FOLDER** to match `var.workspace_folder` if your app resolves workspace paths. Effective catalog/schema in UI: **Setup & Run** → **Save catalog & schema**.

## Workspace components ↔ UI mapping

Every Databricks workspace component (bundle resource) is linked from the app so users can run or open it with one click.

| Workspace component | Bundle resource | UI location | One-click action |
|---------------------|-----------------|-------------|------------------|
| Transaction Stream Simulator | `streaming_simulator.transaction_stream_simulator` | **Setup & Run** step 1 | Run simulator / Open job (run) |
| Payment Analysis ETL (Lakeflow) | `pipelines.payment_analysis_etl` | **Setup & Run** step 2 | Start ETL pipeline / Open pipeline |
| Create Gold Views | `ml_jobs.create_gold_views_job` | **Setup & Run** step 3 | Run gold views job / Open job (run) |
| Lakehouse tables (SQL) | — | **Setup & Run** step 4 | Open SQL Warehouse / Explore schema |
| Train ML Models | `ml_jobs.train_ml_models_job` | **Setup & Run** step 5 | Run ML training / Open job (run) |
| Orchestrator Agent | `jobs.orchestrator_agent_job` (agents.yml) | **Setup & Run** step 6 | Run orchestrator / Open job (run) |
| Specialist agents (5) | `jobs.smart_routing_agent_job` etc. (agents.yml) | **Setup & Run** step 6b | Run Smart Routing, Smart Retry, Decline Analyst, Risk Assessor, Performance Recommender / Open job each |
| Real-time pipeline | `pipelines.payment_realtime_pipeline` | **Setup & Run** step 7 | Start real-time pipeline / Open pipeline |
| Continuous Stream Processor | `streaming_simulator.continuous_stream_processor` | **Setup & Run** Quick links | Stream processor (run) |
| Test Agent Framework | `ml_jobs.test_agent_framework_job` | **Setup & Run** Quick links (if job ID set) | Test Agent Framework / All jobs |
| 12 Dashboards | `dashboards.*` | **Dashboards** page | List from `GET /api/dashboards`; click card opens in workspace |
| Notebooks | workspace files | **Notebooks** page + Decisioning / Models / Setup | List from `GET /api/notebooks`; open folder/notebook URL |
| AI agents (catalog) | — | **AI Agents** page | List from `GET /api/agents/agents`; Open Genie, Open agents folder |
| ML models (UC) | Model Registry (post–train job) | **ML Models** page | List from `GET /api/analytics/models`; open Model Registry / MLflow |
| Rules (Lakehouse) | UC table `approval_rules` | **Rules** page | CRUD via `GET/POST/PATCH/DELETE /api/rules` |
| Recommendations / Online features | UC views/tables | **Dashboard**, **Decisioning** | `GET /api/analytics/recommendations`, `GET /api/analytics/online-features` |
| SQL Warehouse | `sql_warehouse.payment_analysis_warehouse` | **Setup** params + Quick links | SQL Warehouse link; warehouse_id in run params |
| Genie | `genie_spaces` (optional) | **AI Agents** | Open Genie to chat |

Job and pipeline IDs come from `GET /api/setup/defaults` (backend `DEFAULT_IDS` or env). Catalog and schema come from the `app_config` table (or env if the table is empty); the UI can update them via **Save catalog & schema** (`PATCH /api/setup/config`). Set `DATABRICKS_JOB_ID_TEST_AGENT_FRAMEWORK` after deploy to enable the Test Agent Framework quick link.

## UI & verification checklist

- **Setup & Run:** Steps 1–7 and 6b; every job and pipeline has a **Run** and **Open** (or equivalent) one-click action. Quick links: Jobs, Pipelines, SQL Warehouse, Explore schema, Genie, Stream processor, Test Agent Framework (if configured), All jobs.
- **Dashboards:** 12 dashboards; list from `GET /api/dashboards`; click card opens dashboard in workspace.
- **AI Agents:** Genie + agent list; Open Genie, Open agents folder.
- **ML Models:** Four models; list from backend; cards open Model Registry or MLflow.
- **Other UI:** Dashboard home, Decisioning, Rules, Notebooks, Declines, Reason codes, Smart checkout, Smart retry; each links to relevant workspace resource or API. Incidents → Real-Time Monitoring; Experiments → MLflow. Profile stays app-only.
- **Verify:** `GET /api/setup/defaults`, `GET /api/dashboards`, `GET /api/analytics/models`, `GET /api/agents/agents`, `GET /api/rules`. Frontend: set `VITE_DATABRICKS_HOST` for Open-in-Databricks links.

---

**See also:** [0_BUSINESS_CHALLENGES](0_BUSINESS_CHALLENGES.md) · [2_DATA_FLOW](2_DATA_FLOW.md) · [3_AGENTS_VALUE](3_AGENTS_VALUE.md) · [5_DEMO_SETUP](5_DEMO_SETUP.md)
