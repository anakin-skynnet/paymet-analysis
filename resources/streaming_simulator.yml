# =============================================================================
# Streaming Jobs - Simulator and Continuous Processor
# =============================================================================
# Transaction stream simulator (synthetic events) and continuous stream processor.
# Catalog/schema and checkpoint path from variables; checkpoint in Unity Catalog volume.
# =============================================================================

resources:
  jobs:
    transaction_stream_simulator:
      name: "[${var.environment}] Transaction Stream Simulator"
      description: "Generates payment events at high volume for real-time pipeline testing"
      tasks:
        - task_key: generate_transactions
          description: "Generate synthetic payment events at high volume"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/streaming/transaction_simulator
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              events_per_second: "1000"
              duration_minutes: "60"
              output_mode: "delta"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            autoscale:
              min_workers: 1
              max_workers: 2
            spark_conf:
              spark.databricks.delta.optimizeWrite.enabled: "true"
              spark.databricks.delta.autoCompact.enabled: "true"
            spark_env_vars:
              EVENTS_PER_SECOND: "1000"
          timeout_seconds: 3600
      tags:
        environment: ${var.environment}
        project: payment-analysis
        component: simulator
        type: streaming
        domain: payment-analytics

    continuous_stream_processor:
      name: "[${var.environment}] Continuous Stream Processor"
      description: "Always-on job that processes streaming payment events in real-time"
      tasks:
        - task_key: process_stream
          description: "Process incoming payment events continuously"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/streaming/continuous_processor
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              checkpoint_location: "/Volumes/${var.catalog}/${var.schema}/checkpoints/stream_processor"
              trigger_interval: "1 second"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            autoscale:
              min_workers: 1
              max_workers: 2
            spark_conf:
              spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled: "true"
              spark.sql.streaming.stateStore.providerClass: "com.databricks.sql.streaming.state.RocksDBStateStoreProvider"
          timeout_seconds: 0
      continuous:
        pause_status: "PAUSED"
      max_concurrent_runs: 1
      tags:
        environment: ${var.environment}
        project: payment-analysis
        component: streaming
        type: continuous
        domain: payment-analytics
