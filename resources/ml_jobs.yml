# =============================================================================
# ML and Analytics Jobs — Consolidated by logical steps (1, 3, 4, 5)
# =============================================================================
# Step 1: Create data repositories (lakehouse, vector search, lakebase).
# Step 3: Initialize ingestion into lakehouse, lakebase, vector search.
# Step 4: Create and deploy Databricks dashboards.
# Step 5: Train models and publish to model serving.
#
# Step 2 (simulate events) → resources/streaming_simulator.yml
# Step 6 (deploy agents)   → resources/agents.yml
#
# Execution order: 1 → 2 → 3 → 4 → 5 → 6. See docs/DEPLOYMENT_GUIDE.md and Setup & Run.
# =============================================================================

resources:
  jobs:
    # -------------------------------------------------------------------------
    # 1. Create data repositories: catalog/schema → Lakebase → lakehouse → vector search
    # -------------------------------------------------------------------------
    "job_1_create_data_repositories":
      name: "[${var.environment}] 1. Create Data Repositories (Lakehouse, Vector Search, Lakebase)"
      description: "Ensure catalog and schema; initialize Lakebase (default config and rules); create lakehouse tables; vector search endpoint and index. Run once after bundle deploy."
      tasks:
        - task_key: ensure_catalog_schema
          description: "Create Unity Catalog and schema if they do not exist"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/transform/run_ensure_catalog_schema
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 120
          max_retries: 2

        - task_key: lakebase_data_init
          description: "Initialize Lakebase Postgres with default app_config and approval_rules"
          depends_on:
            - task_key: ensure_catalog_schema
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/transform/run_lakebase_data_init
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              lakebase_instance_name: ${var.lakebase_instance_name}
              lakebase_database_name: ${var.lakebase_database_name}
              lakebase_schema: "app"
              warehouse_id: ${var.warehouse_id}
              default_events_per_second: "1000"
              default_duration_minutes: "60"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 300
          max_retries: 2

        - task_key: lakehouse_bootstrap
          description: "Create Lakehouse tables, views, and seed data for Rules, Decisioning, and Dashboard"
          depends_on:
            - task_key: lakebase_data_init
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/transform/run_lakehouse_bootstrap
            base_parameters:
              workspace_path: ${workspace.file_path}
              catalog: ${var.catalog}
              schema: ${var.schema}
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 300
          max_retries: 2

        - task_key: create_vector_search_index
          description: "Create vector search endpoint and delta-sync index for similar-transaction lookup"
          depends_on:
            - task_key: lakehouse_bootstrap
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/vector_search/create_index
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              environment: ${var.environment}
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 1200
          max_retries: 1
      schedule:
        quartz_cron_expression: "0 0 1 1 1 ?"
        timezone_id: "UTC"
        pause_status: "PAUSED"
      tags:
        environment: ${var.environment}
        project: payment-analysis
        component: data-repositories
        step: "1"

    # -------------------------------------------------------------------------
    # 3. Initialize ingestion: real-time ingestion into lakehouse, lakebase, vector search
    # -------------------------------------------------------------------------
    "job_3_initialize_ingestion":
      name: "[${var.environment}] 3. Initialize Ingestion (Lakehouse, Lakebase, Vector Search)"
      description: "Create gold views and sync vector search index. Ingests data into lakehouse (views), and vector search (delta-sync). Run after step 2 (simulator) or when pipelines have produced silver data."
      tasks:
        - task_key: create_gold_views
          description: "Create gold-layer analytical views for dashboards and Genie"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/transform/run_gold_views
            base_parameters:
              workspace_path: ${workspace.file_path}
              catalog: ${var.catalog}
              schema: ${var.schema}
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 600
          max_retries: 2

        - task_key: sync_vector_search
          description: "Sync vector search index from transaction_summaries_for_search"
          depends_on:
            - task_key: create_gold_views
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/vector_search/create_index
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              environment: ${var.environment}
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 600
          max_retries: 1
      schedule:
        quartz_cron_expression: "0 30 * * * ?"
        timezone_id: "UTC"
        pause_status: "PAUSED"
      tags:
        environment: ${var.environment}
        project: payment-analysis
        component: ingestion
        step: "3"

    # -------------------------------------------------------------------------
    # 4. Create and deploy Databricks dashboards
    # -------------------------------------------------------------------------
    "job_4_deploy_dashboards":
      name: "[${var.environment}] 4. Deploy Dashboards (Prepare & Publish)"
      description: "Prepare dashboard assets (catalog/schema substitution) and publish all AI/BI dashboards with embed credentials for the app UI."
      tasks:
        - task_key: prepare_dashboards
          description: "Prepare dashboard assets (catalog/schema substitution)"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/transform/prepare_dashboards
            base_parameters:
              workspace_path: ${workspace.file_path}
              catalog: ${var.catalog}
              schema: ${var.schema}
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 600
          max_retries: 2

        - task_key: publish_dashboards
          description: "Publish all AI/BI dashboards with embed credentials"
          depends_on:
            - task_key: prepare_dashboards
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/transform/publish_dashboards
            base_parameters:
              dashboards_path: ${workspace.root_path}/dashboards
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 1200
          max_retries: 1
      schedule:
        quartz_cron_expression: "0 0 12 * * ?"
        timezone_id: "UTC"
        pause_status: "PAUSED"
      tags:
        environment: ${var.environment}
        project: payment-analysis
        component: dashboards
        step: "4"

    # -------------------------------------------------------------------------
    # 5. Train models and publish to model serving
    # -------------------------------------------------------------------------
    "job_5_train_models_and_serving":
      name: "[${var.environment}] 5. Train Models & Publish to Model Serving"
      description: "Train approval propensity, risk scoring, smart routing, and smart retry models; register in Unity Catalog. Publish endpoints via model_serving.yml after this job."
      tasks:
        - task_key: train_models
          description: "Train all ML models with MLflow tracking"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/ml/train_models
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
          new_cluster:
            spark_version: "15.4.x-cpu-ml-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            spark_env_vars:
              MLFLOW_TRACKING_URI: "databricks"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 5400
          max_retries: 1
      schedule:
        quartz_cron_expression: "0 0 3 * * ?"
        timezone_id: "UTC"
        pause_status: "PAUSED"
      tags:
        environment: ${var.environment}
        project: payment-analysis
        component: ml-training
        step: "5"
