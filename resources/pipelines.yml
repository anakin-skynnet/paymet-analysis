# =============================================================================
# Lakeflow — Pipelines 8 & 9 (run after jobs 1–7 when needed)
# =============================================================================
# Two pipelines: main ETL (Bronze → Silver → Gold) and dedicated real-time stream.
# Numeric prefix matches execution order (jobs 1–7, then pipelines 8, 9).
# Catalog/schema from variables. Serverless, continuous, Photon. Development mode in dev.
# =============================================================================

resources:
  pipelines:
    # 8. Main ETL: bronze_ingest → silver_transform → gold_views (notebooks)
    payment_analysis_etl:
      name: "[${var.environment}] 8. Payment Analysis ETL"
      catalog: ${var.catalog}
      schema: ${var.schema}
      channel: CURRENT
      configuration:
        catalog_name: ${var.catalog}
        schema_name: ${var.schema}
        # Delta and schema evolution
        spark.databricks.delta.preview.enabled: "true"
        spark.databricks.delta.schema.autoMerge.enabled: "true"
        pipelines.tableMaterialization.autoPropagate: "true"
        # Write and compaction
        spark.databricks.delta.optimizeWrite.enabled: "true"
        spark.databricks.delta.autoCompact.enabled: "true"
        pipelines.trigger.interval: "2 seconds"
        pipelines.enableAutoOptimize: "true"
      libraries:
        - notebook:
            path: ${workspace.root_path}/src/payment_analysis/streaming/bronze_ingest
        - notebook:
            path: ${workspace.root_path}/src/payment_analysis/transform/silver_transform
        - notebook:
            path: ${workspace.root_path}/src/payment_analysis/transform/gold_views
      serverless: true
      continuous: true
      development: ${var.pipelines_development}
      photon: true

    # 9. Dedicated real-time pipeline (single notebook). Continuous streaming (not triggered).
    payment_realtime_pipeline:
      name: "[${var.environment}] 9. Payment Real-Time Stream"
      catalog: ${var.catalog}
      schema: ${var.schema}
      channel: CURRENT
      configuration:
        catalog_name: ${var.catalog}
        schema_name: ${var.schema}
        spark.databricks.delta.optimizeWrite.enabled: "true"
        spark.databricks.delta.autoCompact.enabled: "true"
        pipelines.enableAutoOptimize: "true"
      libraries:
        - notebook:
            path: ${workspace.root_path}/src/payment_analysis/streaming/realtime_pipeline
      serverless: true
      continuous: true
      development: false
      photon: true
