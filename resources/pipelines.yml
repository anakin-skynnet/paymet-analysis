# Delta Live Tables Pipelines
# Uses latest DLT features: serverless, enhanced autoscaling, materialized views

resources:
  pipelines:
    # ==========================================================================
    # Batch ETL Pipeline (Bronze → Silver → Gold)
    # ==========================================================================
    payment_analysis_etl:
      name: "[${var.environment}] Payment Analysis ETL"
      
      # Unity Catalog integration
      catalog: ${var.catalog}
      target: payment_analysis_${var.environment}
      
      # Use CURRENT channel for latest DLT features
      channel: CURRENT
      
      # Pipeline configuration
      configuration:
        # Schema configuration
        catalog_name: ${var.catalog}
        schema_name: payment_analysis_${var.environment}
        # Performance optimizations
        spark.databricks.delta.preview.enabled: "true"
        spark.databricks.delta.schema.autoMerge.enabled: "true"
        # DLT optimizations
        pipelines.tableMaterialization.autoPropagate: "true"
      
      # Pipeline libraries (notebooks)
      libraries:
        - notebook:
            path: ${workspace.file_path}/src/payment_analysis/streaming/bronze_ingest
        - notebook:
            path: ${workspace.file_path}/src/payment_analysis/transform/silver_transform
        - notebook:
            path: ${workspace.file_path}/src/payment_analysis/transform/gold_views
      
      # Serverless compute (recommended for cost optimization)
      serverless: true
      
      # Batch mode for scheduled ETL
      continuous: false
      
      # Development mode (set false in prod target)
      development: true
      
      # Enable Photon acceleration
      photon: true

    # ==========================================================================
    # Real-Time Streaming Pipeline
    # ==========================================================================
    payment_realtime_pipeline:
      name: "[${var.environment}] Payment Real-Time Stream"
      
      # Unity Catalog integration
      catalog: ${var.catalog}
      target: payment_analysis_${var.environment}
      
      channel: CURRENT
      
      # Streaming-optimized configuration
      configuration:
        catalog_name: ${var.catalog}
        schema_name: payment_analysis_${var.environment}
        # Streaming optimizations
        spark.databricks.delta.optimizeWrite.enabled: "true"
        spark.databricks.delta.autoCompact.enabled: "true"
        spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled: "true"
        # Low-latency settings
        pipelines.trigger.interval: "1 second"
        pipelines.enableAutoOptimize: "true"
      
      libraries:
        - notebook:
            path: ${workspace.file_path}/src/payment_analysis/streaming/realtime_pipeline
      
      # Use dedicated cluster for streaming (serverless not supported for continuous)
      clusters:
        - label: default
          autoscale:
            min_workers: 1
            max_workers: 4
            mode: ENHANCED
          spark_conf:
            spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled: "true"
            spark.sql.streaming.stateStore.providerClass: "com.databricks.sql.streaming.state.RocksDBStateStoreProvider"
      
      # Continuous mode for real-time processing
      continuous: true
      
      development: true
      
      photon: true
