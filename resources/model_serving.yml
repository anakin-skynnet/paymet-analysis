# =============================================================================
# Model Serving Endpoints — REFERENCE / OPTIONAL
# =============================================================================
# This file is OPTIONAL and included primarily for documentation and for
# managing AI Gateway configurations via DAB.
#
# ** Endpoints are now created/updated PROGRAMMATICALLY by Jobs 5 and 6: **
#   - Job 5 (train_models.py): creates/updates 4 ML model endpoints
#   - Job 6 (agentbricks_register.py / register_responses_agent.py):
#     creates/updates agent endpoints (orchestrator, decline-analyst)
#
# This means the solution is portable to ANY workspace: run Jobs 1-7 and
# all endpoints are created automatically with the correct model versions.
# No hardcoded entity_version needed.
#
# If you ALSO want DAB to manage endpoint config (e.g. AI Gateway, workload size),
# you can include this file in databricks.yml. Set the entity_version variables
# AFTER running Jobs 5 & 6 so they match the registered model versions.
#
# Tiered LLM strategy:
#   Orchestrator → llm_endpoint_orchestrator (Claude Opus 4.6, strongest reasoning)
#   Specialists  → llm_endpoint_specialist   (Claude Sonnet 4.5, balanced speed/quality)
#   Simple tasks → llm_endpoint_simple        (Llama 3.3 70B, fast/cheap)
#
# Agent endpoints use ResponsesAgent (MLflow 3.x) with lazy LangGraph initialization.
# See: https://mlflow.org/docs/latest/genai/flavors/responses-agent-intro/
# =============================================================================

resources:
  model_serving_endpoints:
    # Supervisor / Orchestrator (ResponsesAgent wrapping LangGraph multi-agent).
    payment_analysis_orchestrator_endpoint:
      name: "payment-analysis-orchestrator"
      config:
        served_entities:
          - entity_name: "${var.catalog}.agents.orchestrator"
            entity_version: "${var.orchestrator_model_version}"
            workload_size: "Small"
            scale_to_zero_enabled: true
            workload_type: "CPU"
            environment_vars:
              ENABLE_MLFLOW_TRACING: "true"
              CATALOG: "${var.catalog}"
              SCHEMA: "${var.schema}"
              LLM_ENDPOINT: "${var.llm_endpoint_orchestrator}"
              LLM_ENDPOINT_ORCHESTRATOR: "${var.llm_endpoint_orchestrator}"
              LLM_ENDPOINT_SPECIALIST: "${var.llm_endpoint_specialist}"
        traffic_config:
          routes: [{ served_model_name: "orchestrator-${var.orchestrator_model_version}", traffic_percentage: 100 }]

    # Decline analyst agent (ResponsesAgent wrapping LangGraph ReAct + UC tools)
    decline_analyst_endpoint:
      name: "decline-analyst"
      config:
        served_entities:
          - entity_name: "${var.catalog}.agents.decline_analyst"
            entity_version: "${var.decline_analyst_model_version}"
            workload_size: "Small"
            scale_to_zero_enabled: true
            workload_type: "CPU"
            environment_vars:
              ENABLE_MLFLOW_TRACING: "true"
              CATALOG: "${var.catalog}"
              SCHEMA: "${var.schema}"
              LLM_ENDPOINT: "${var.llm_endpoint_specialist}"
        traffic_config:
          routes: [{ served_model_name: "decline_analyst-${var.decline_analyst_model_version}", traffic_percentage: 100 }]

    # Predicts transaction approval probability
    approval_propensity_endpoint:
      name: "approval-propensity"
      config:
        served_entities:
          - entity_name: "${var.catalog}.${var.schema}.approval_propensity_model"
            entity_version: "${var.ml_model_version}"
            workload_size: "Small"
            scale_to_zero_enabled: true
            workload_type: "CPU"
            environment_vars: { ENABLE_MLFLOW_TRACING: "true" }
        traffic_config:
          routes: [{ served_model_name: "approval_propensity_model-${var.ml_model_version}", traffic_percentage: 100 }]
      ai_gateway:
        rate_limits: [{ key: "user", renewal_period: "minute", calls: 100 }]
        usage_tracking_config: { enabled: true }

    # Fraud risk scoring
    risk_scoring_endpoint:
      name: "risk-scoring"
      config:
        served_entities:
          - entity_name: "${var.catalog}.${var.schema}.risk_scoring_model"
            entity_version: "${var.ml_model_version}"
            workload_size: "Small"
            scale_to_zero_enabled: true
            workload_type: "CPU"
            environment_vars: { ENABLE_MLFLOW_TRACING: "true" }
        traffic_config:
          routes: [{ served_model_name: "risk_scoring_model-${var.ml_model_version}", traffic_percentage: 100 }]
      ai_gateway:
        rate_limits: [{ key: "user", renewal_period: "minute", calls: 100 }]
        usage_tracking_config: { enabled: true }

    # Optimal payment route recommendation
    smart_routing_endpoint:
      name: "smart-routing"
      config:
        served_entities:
          - entity_name: "${var.catalog}.${var.schema}.smart_routing_policy"
            entity_version: "${var.ml_model_version}"
            workload_size: "Small"
            scale_to_zero_enabled: true
            workload_type: "CPU"
        traffic_config:
          routes: [{ served_model_name: "smart_routing_policy-${var.ml_model_version}", traffic_percentage: 100 }]
      ai_gateway:
        rate_limits: [{ key: "user", renewal_period: "minute", calls: 200 }]
        usage_tracking_config: { enabled: true }

    # Retry success likelihood and recovery
    smart_retry_endpoint:
      name: "smart-retry"
      config:
        served_entities:
          - entity_name: "${var.catalog}.${var.schema}.smart_retry_policy"
            entity_version: "${var.ml_model_version}"
            workload_size: "Small"
            scale_to_zero_enabled: true
            workload_type: "CPU"
        traffic_config:
          routes: [{ served_model_name: "smart_retry_policy-${var.ml_model_version}", traffic_percentage: 100 }]
      ai_gateway:
        rate_limits: [{ key: "user", renewal_period: "minute", calls: 200 }]
        usage_tracking_config: { enabled: true }
